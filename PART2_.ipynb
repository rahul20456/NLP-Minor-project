{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMxG-7PCP6SJ",
        "outputId": "15edd176-0512-47cf-fcd9-03ffbaddc823"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /Users/apple/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to\n",
            "[nltk_data]     /Users/apple/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#import the libraries\n",
        "import pandas as pd\n",
        "import tqdm as tqdm\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "# from textblob import TextBlob\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "nltk.download('stopwords')\n",
        "import matplotlib.pyplot as plt\n",
        "# import wordcloud\n",
        "# from wordcloud import WordCloud\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import itertools\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pickle\n",
        "from numpy.random import choice\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkCAhpJFP6SN"
      },
      "outputs": [],
      "source": [
        "#importing data and initialising stopwords\n",
        "data = pd.read_csv(\"A1_dataset.csv\")\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoKsiyatP6SN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amjltGsCP6SN"
      },
      "outputs": [],
      "source": [
        "#initializing punctuations\n",
        "punct = string.punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yb2YnCMPP6SO"
      },
      "outputs": [],
      "source": [
        "#preprocessing\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def remove_white_spaces(sent):\n",
        "    x = re.sub(r'\\s+',' ',sent)#will remove extra spaces\n",
        "    return(x)\n",
        "def remove_unwanted_characters(sent):\n",
        "    pat = re.compile('@[^\\s]+')#atleast one character should be present after @ and valid username contains alphabets,digits and _\n",
        "    cl_pat = re.sub(pat,'',sent)\n",
        "    pat = re.compile(\"www.\\S+\")\n",
        "    cl_pat = re.sub(pat, ' ', cl_pat)\n",
        "    pat = re.compile(\"<.*?>\")#html\n",
        "    cl_pat = re.sub(pat, ' ', cl_pat)\n",
        "    pat = re.compile(\"https?://\\S+\")#url\n",
        "    cl_pat = re.sub(pat, ' ', cl_pat)\n",
        "    cl_pat = ''.join([word for word in cl_pat if not word.isdigit()])\n",
        "    cl_pat = re.sub(r\"(\\!)\\1+\", ' ! ', cl_pat)\n",
        "    cl_pat = re.sub(r\"(\\?)\\1+\", ' ? ', cl_pat)\n",
        "    cl_pat = re.sub(r\"(\\.)\\1+\", ' \\. ', cl_pat)\n",
        "    cl_pat = re.sub(r\"(\\\\)\\1+\",\"\",cl_pat)\n",
        "    cl_pat = re.sub(r\"\\\\\",\"\",cl_pat)\n",
        "    cl_pat = word_tokenize(cl_pat)\n",
        "    words = []\n",
        "    for word in cl_pat:\n",
        "        word = word.lower()\n",
        "        if(word in stop_words or word in punct):\n",
        "            continue\n",
        "        words.append(word)\n",
        "    \n",
        "    cl_pat = \" \".join(words)\n",
        "    cl_pat = re.sub(r\"we 're\", \"we are\", cl_pat)\n",
        "    cl_pat = re.sub(r\"i \\'m\", \"i am\", cl_pat)\n",
        "    cl_pat = re.sub(r\"it \\'s\", \"it is\", cl_pat)\n",
        "    cl_pat = re.sub(r\"ca n\\'t\", \"can't\", cl_pat)\n",
        "    cl_pat = re.sub(r\"do n\\'t\", \"don't\", cl_pat)\n",
        "    cl_pat = re.sub(r\"\\'m not\", \"i am not\", cl_pat)\n",
        "    cl_pat = re.sub(r\"we 're\", \"we are\", cl_pat)\n",
        "    cl_pat = re.sub(r\"wo n\\'t\", \"will not\", cl_pat)\n",
        "    cl_pat = re.sub(r\"\\'s a\", \"is a\", cl_pat)\n",
        "    cl_pat = re.sub(r\"does n\\'t\", \"doesn't\", cl_pat)\n",
        "    cl_pat = re.sub(r\"do n\\'t\", \"don't\", cl_pat)\n",
        "    cl_pat = re.sub(r\"\\'ll be\", \"i will be\", cl_pat)\n",
        "    cl_pat = re.sub(r\"gon na\", \"gonna\", cl_pat)\n",
        "    cl_pat = re.sub(r\"got ta\", \"gotta\", cl_pat)\n",
        "    cl_pat = re.sub(r\"\\wan na \", \"wanna\", cl_pat)\n",
        "#     cl_pat = re.sub(r\"\\'ll be\", \"i will be\", cl_pat)\n",
        "    \n",
        "\n",
        "    return cl_pat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qk4aWiOiP6SO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7AgiirmP6SO",
        "outputId": "8a7d9150-b7fb-40d8-efac-b277aadd8bb3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████| 4287/4287 [00:01<00:00, 2145.06it/s]\n"
          ]
        }
      ],
      "source": [
        "#preprocessing the training dataa\n",
        "for i in tqdm.tqdm(data.index):\n",
        "    data.loc[i,'TEXT'] = remove_unwanted_characters(data.loc[i,'TEXT'])\n",
        "    data.loc[i,'TEXT'] = remove_white_spaces(data.loc[i,'TEXT'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hR-NRVfBP6SP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFW3S_liP6SP",
        "outputId": "10436944-2df3-49da-d1ca-7a1b99490583"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████| 4287/4287 [00:00<00:00, 10657.20it/s]\n"
          ]
        }
      ],
      "source": [
        "#adding starters and enders\n",
        "for i in tqdm.tqdm(data.index):\n",
        "    data.loc[i,'TEXT'] ='<s> ' + data.loc[i,'TEXT'] +' </s>'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qgfqv46CP6SQ",
        "outputId": "14a94507-4ca0-4f32-a8c8-3f1cb726ab86"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████| 4287/4287 [00:00<00:00, 71053.38it/s]\n"
          ]
        }
      ],
      "source": [
        "#Finding the unigram count of the words in training corpus\n",
        "unigrams = []\n",
        "unigram_count = dict()\n",
        "\n",
        "for i in tqdm.tqdm(data.index):\n",
        "    sent = data.loc[i,'TEXT']\n",
        "    tok = sent.split(' ')\n",
        "    for i in range(len(tok)):\n",
        "        \n",
        "        try:\n",
        "            unigram_count[tok[i]]+=1\n",
        "        except:\n",
        "            unigram_count[tok[i]] = 1\n",
        "\n",
        "\n",
        "unigrams = list(unigram_count.keys())\n",
        "for i in unigrams:\n",
        "    if i in punct:\n",
        "        unigrams.remove(i)\n",
        "    if(i==\"''\"):\n",
        "        unigrams.remove(i)\n",
        "vocab = set(unigrams)\n",
        "V = len(unigrams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPSBzvcFP6SQ",
        "outputId": "7ffd5e3c-8601-4574-fdad-fbd174e8bdba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████| 4287/4287 [00:00<00:00, 48998.64it/s]\n"
          ]
        }
      ],
      "source": [
        "#Finding the bigram count of the words in training corpus\n",
        "bigrams = []\n",
        "bigram_count = dict()\n",
        "\n",
        "for i in tqdm.tqdm(data.index):\n",
        "    sent = data.loc[i,'TEXT']\n",
        "    tok = sent.split(' ')\n",
        "    for i in range(len(tok)-1):\n",
        "        if(tok[i] ==\"''\" or tok[i+1]==\"''\"):\n",
        "            continue\n",
        "        try:\n",
        "            bigram_count[(tok[i],tok[i+1])] +=1\n",
        "        except:\n",
        "            bigram_count[(tok[i],tok[i+1])] = 1\n",
        "\n",
        "bigrams = set(list(bigram_count.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Zn-RGf2P6SQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBax7couP6SQ"
      },
      "outputs": [],
      "source": [
        "#unsmoothed bigram probabilites\n",
        "\n",
        "unsmoothed = dict()\n",
        "\n",
        "for i in bigrams:\n",
        "    countb = bigram_count[i]#total number of bigram count \n",
        "    countu = unigram_count[i[0]]#total number of unigram count of previous word\n",
        "    unsmoothed[i] = countb/countu\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PesKyjO4P6SR"
      },
      "outputs": [],
      "source": [
        "#smoothed bigram probability of the bigrams whose count is not equal to 0\n",
        "smoothed = dict()\n",
        "for i in bigrams:\n",
        "    countb_s = bigram_count[i] + 1\n",
        "    countu_s = unigram_count[i[0]]+V\n",
        "    smoothed[i] = countb_s/countu_s\n",
        "    \n",
        "helper = dict()\n",
        "for i in vocab:\n",
        "    helper[i] = 1/(unigram_count[i]+V)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buvwsT-cP6SR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bMUW1_MP6SR",
        "outputId": "55989304-93f3-4f40-a1a0-57baa24a7f69"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████████████████████████████████| 7950/7950 [00:59<00:00, 133.84it/s]\n"
          ]
        }
      ],
      "source": [
        "#building the laplace model\n",
        "laplace = dict()\n",
        "for v in tqdm.tqdm(list(vocab)):\n",
        "    for v1 in list(vocab):\n",
        "        big = (v,v1)\n",
        "        if(big in bigrams):\n",
        "            laplace[big]= smoothed[big]#if the bigram exists in the corpus\n",
        "        else:\n",
        "            laplace[big] = helper[big[0]]#if the bigram doesn't exist then laplace depends only on previous word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybRz6lZvP6SR"
      },
      "source": [
        "## Save smoothed bigram model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQtGTEB9P6SS"
      },
      "outputs": [],
      "source": [
        "# saving the smoothed bigram model\n",
        "\n",
        "\n",
        "with open('laplace_final.pickle', 'wb') as handle:\n",
        "    pickle.dump(laplace, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8RGCSfxP6ST"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoC10ms2P6ST"
      },
      "outputs": [],
      "source": [
        "# laplace = unserialized_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwKzMjUsP6ST",
        "outputId": "39832fae-df00-4c85-97cd-8298ff017580"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "63202500"
            ]
          },
          "execution_count": 200,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(laplace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8obYldjP6ST"
      },
      "outputs": [],
      "source": [
        "#Segregating the positive and negative data\n",
        "datapos = data[data['LABEL']==1]\n",
        "dataneg = data[data['LABEL']==0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Awh7PP37P6ST"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "l8lzSvsdP6ST"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lu-Gazd9P6ST",
        "outputId": "f06db2af-48a3-4ebd-ea3d-aa1725a8da95"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████| 2287/2287 [00:00<00:00, 233317.28it/s]\n",
            "100%|███████████████████████████████████| 2287/2287 [00:00<00:00, 113748.05it/s]\n"
          ]
        }
      ],
      "source": [
        "#finding the unigram count and bigram count of the words in positive data i.e. datapos\n",
        "pos_unigram = dict()\n",
        "pos_bigram = dict()\n",
        "for i in tqdm.tqdm(datapos['TEXT']):\n",
        "    token = i.split(' ')\n",
        "    for i in token:\n",
        "        if(i==\"''\"):#useless word removed\n",
        "            continue\n",
        "        try:\n",
        "            pos_unigram[i]+=1\n",
        "        except:\n",
        "            pos_unigram[i] = 1\n",
        "            \n",
        "            \n",
        "for i in tqdm.tqdm(datapos['TEXT']):\n",
        "    token = i.split(' ')\n",
        "    for i in range(len(token)-1):\n",
        "        if(i == \"''\"): #useless word removed\n",
        "            continue\n",
        "        try:\n",
        "            pos_bigram[(token[i],token[i+1])]+=1\n",
        "        except:\n",
        "            pos_bigram[(token[i],token[i+1])] = 1\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aza22jjDP6SU",
        "outputId": "c3b14148-b796-4aa9-8abc-18a014f6a13d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 205045.29it/s]\n",
            "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 106010.46it/s]\n"
          ]
        }
      ],
      "source": [
        "#finding the unigram and bigram count of the words in negative data i.e. dataneg\n",
        "neg_unigram = dict()\n",
        "neg_bigram = dict()\n",
        "for i in tqdm.tqdm(dataneg['TEXT']):\n",
        "    token = i.split(' ')\n",
        "    for i in token:\n",
        "        if(i==\"''\"):\n",
        "            continue\n",
        "        try:\n",
        "            neg_unigram[i]+=1\n",
        "        except:\n",
        "            neg_unigram[i] = 1\n",
        "            \n",
        "            \n",
        "for i in tqdm.tqdm(dataneg['TEXT']):\n",
        "    token = i.split(' ')\n",
        "    for i in range(len(token)-1):\n",
        "        if(i==\"''\"):\n",
        "            continue\n",
        "        try:\n",
        "            neg_bigram[(token[i],token[i+1])]+=1\n",
        "        except:\n",
        "            neg_bigram[(token[i],token[i+1])] = 1\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sF32_sDhP6SU"
      },
      "outputs": [],
      "source": [
        "# saving the positive and negative bigram model\n",
        "\n",
        "\n",
        "\n",
        "with open('positive_bigram.pickle', 'wb') as handle:\n",
        "    pickle.dump(pos_bigram, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('negative_bigram.pickle', 'wb') as handle:\n",
        "    pickle.dump(neg_bigram, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "4ooOTQLxP6SU"
      },
      "outputs": [],
      "source": [
        "#storing the starting words in negative  and positive sentiment sentences in training corpus\n",
        "start_pos = dict()\n",
        "\n",
        "for i in datapos['TEXT']:\n",
        "    try:\n",
        "        start_pos[i.split()[1]] = pos_unigram[i.split()[1]]/unigram_count[i.split()[1]]\n",
        "    except:\n",
        "        pass\n",
        "        \n",
        "\n",
        "start_neg = dict()\n",
        "\n",
        "for i in dataneg['TEXT']:\n",
        "    start_neg[i.split()[1]] = neg_unigram[i.split()[1]]/unigram_count[i.split()[1]]\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5jlaC99P6SU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_s4vFjqnP6SU"
      },
      "outputs": [],
      "source": [
        "# S_pos = list(dict(sorted(start_pos.items(), key = lambda item : item[1], reverse=True)).keys())\n",
        "# S_neg = list(dict(sorted(start_neg.items(), key = lambda item : item[1], reverse=True)).keys())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UA0V5GdP6SV"
      },
      "outputs": [],
      "source": [
        "#\n",
        "def candidate_words(prev):\n",
        "    candidates = dict()\n",
        "    \n",
        "    for i in list(bigrams):\n",
        "        # print(i[0])\n",
        "        if(prev==i[0]):\n",
        "            candidates[i] = laplace[i]\n",
        "    \n",
        "    candidates = dict(sorted(candidates.items(),key = lambda item:item[1],reverse = True))\n",
        "    \n",
        "    return candidates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8yI9diNP6SV"
      },
      "outputs": [],
      "source": [
        "def best_word(cw,ind,sentiment,prev):\n",
        "    if(sentiment=='pos'):\n",
        "        ugram = pos_unigram\n",
        "        bgram = pos_bigram\n",
        "    else:\n",
        "        ugram = neg_unigram\n",
        "        bgram = neg_bigram\n",
        "        \n",
        "    count = 0\n",
        "    final_prob = dict()\n",
        "        \n",
        "    for i in cw.keys():\n",
        "        if(count >=10):\n",
        "            break\n",
        "        else:\n",
        "            count=count +1\n",
        "        n1 = bigram_count[(prev,i[1])]\n",
        "        try:\n",
        "            n2 = bgram[(prev,i[1])]\n",
        "        except:\n",
        "            n2=0\n",
        "            \n",
        "        d1 = unigram_count[prev]\n",
        "        try:\n",
        "            d2 = ugram[prev]\n",
        "        except:\n",
        "            d2 = d1\n",
        "        final_prob[i] = (n1+n2+1)/(d1+d2+V)\n",
        "    \n",
        "    B = dict(sorted(final_prob.items(),key = lambda item:item[1],reverse = True))\n",
        "\n",
        "    return B\n",
        "    \n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-xtIG3zP6SV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-tqYOFlP6SV"
      },
      "outputs": [],
      "source": [
        "def generating_sentences():\n",
        "    final = []\n",
        "    # sent = ['<s>']\n",
        "    \n",
        "    j = 0\n",
        "    start_words = list()\n",
        "    while len(final)<500:\n",
        "\n",
        "        if(len(final)!=j):\n",
        "            j-=1\n",
        "        sent = ['<s>'] #start of the sentence\n",
        "        num_words = 1\n",
        "        sentiment = random.choice(['pos','neg'])#randomly choose which sentiment sentence should be generated\n",
        "        for i in range(0,24):\n",
        "\n",
        "            if(sent[-1]=='<s>'): \n",
        "                if(sentiment=='pos'):\n",
        "                    x = random.choice(S_pos)#randomly choose a starting word from the list of positive sentiment start word list\n",
        "                    sent.append(x)\n",
        "                else:\n",
        "                    y = random.choice(S_neg)#randomly choose a starting word from the list of negative sentiment start word list\n",
        "                    sent.append(y)\n",
        "                continue\n",
        "            cw = candidate_words(sent[-1]) #sorted dictionary of all bigrams with previous word = the last word added to the sentence\n",
        "            next_word = best_word(cw,i,sentiment,sent[-1])#getting the dictionary of words with beta probability\n",
        "            k = list(next_word.keys())[:2] #slicing the dictionary to get the best 2 words\n",
        "            v = list(next_word.values())[:2]\n",
        "\n",
        "            try:\n",
        "            \n",
        "                index = random.randint(0,len(k)) #choosing a random word out of the best 2 words\n",
        "                nw = k[index][1]\n",
        "\n",
        "            except:\n",
        "                continue \n",
        "            if(nw == '</s>' and num_words<12): # if the word found is </s> but the number of words <12(lower bound) then add more words\n",
        "                i-=1\n",
        "                continue\n",
        "            elif(nw=='</s>'):\n",
        "\n",
        "                p = \" \".join(sent[1:]) # end sentence and add it to final list\n",
        "                final.append(p)\n",
        "                break\n",
        "            else:\n",
        "                num_words+=1 #store the number of words added so far\n",
        "                sent.append(nw) #append word to sentences\n",
        "\n",
        "        \n",
        "    return final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97ueajjuP6SV"
      },
      "outputs": [],
      "source": [
        "# sentences = generating_sentences() \n",
        "#retrieve already generated sentences to save time\n",
        "sent = pd.read_csv(\"sentences.csv\")\n",
        "sentences = sent['sentences']\n",
        "pred = sent['labels']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHvINXTAP6SW",
        "outputId": "0ce01582-ae1d-449e-b998-4e04606d2008"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "execution_count": 209,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KREVO2pwP6SW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woOM_FemP6SW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4kifJ7ZP6SW"
      },
      "source": [
        "### 5.B Save the generated 500 sentences and their sentiment labels in csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqrSwC_JP6SW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "prim = []\n",
        "# sec = []\n",
        "for i in sentences:\n",
        "    sec = []\n",
        "    sec.append(i)\n",
        "    prim.append(sec)\n",
        "\n",
        "file = open('sentences.csv','w+',newline = '')\n",
        "header = [\"sentences\"]\n",
        "\n",
        "with file:\n",
        "    write = csv.writer(file)\n",
        "    write.writerow(header)\n",
        "    write.writerows(prim)\n",
        "\n",
        "X = pd.read_csv(\"sentences.csv\")\n",
        "X['labels'] = pred\n",
        "X.to_csv(\"sentences.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhGyhmJgP6SW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwn0M8gfP6SW"
      },
      "outputs": [],
      "source": [
        "pred = []\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "for i in sentences:\n",
        "    polarity = (sid.polarity_scores(i)['compound'])\n",
        "    if(polarity<0):\n",
        "        pred.append(0)\n",
        "    else:\n",
        "        pred.append(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ty1M0rwZP6SW"
      },
      "source": [
        "## Report the Top -4 bigrams and their scores after smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFBBFWq8P6SX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avaLtdV4P6SX"
      },
      "source": [
        "### 5.c Report the average Perplexity of the generated 500 sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e98mdXo5P6SY"
      },
      "source": [
        "## B.d Report 10 generated samples 5 positive + 5 negatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIWbhRwHP6SY",
        "outputId": "d9cb44f2-5dc6-486b-ea22-3e83e97ce6a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "weeeell sure 'm going bed soon 'll see 'm sorry hear 're still\n",
            "luving sunshine little bit disappointed weather today 'm going back work tomorrow hopefully\n",
            "lmfaooo would n't think 'm sorry hear 're going sleep night sweet\n",
            "friday night friends 'm going back work tomorrow hopefully 'll take care much\n",
            "hayfever medication n't get ready work tomorrow hopefully 'll take care much\n"
          ]
        }
      ],
      "source": [
        "count = 0\n",
        "for i in list(zip(pred,sentences)):\n",
        "    if(count == 5):\n",
        "        break\n",
        "    if(i[0] ==1):\n",
        "        print(i[1])\n",
        "        count+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Xks2NCYP6SY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldDclV3QP6SY",
        "outputId": "b826ba17-6f20-40b9-9e80-2bcea55c2eb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "busy day 'm sorry hear 're going bed soon 'll try go\n",
            "hates hates hates hates hates hates hates hates hates hates hates hates hates\n",
            "despite last day 'm sorry hear 're going bed soon 'll try go\n",
            "sushi 've missed tweets set today 'm going bed soon 'll see\n",
            "whedonverse bloody wales however may pdt ñ load updates seems working hard\n"
          ]
        }
      ],
      "source": [
        "#negative sentiment sentences\n",
        "count = 0\n",
        "for i in list(zip(pred,sentences)):\n",
        "    if(count == 5):\n",
        "        break\n",
        "    if(i[0] ==0):\n",
        "        print(i[1])\n",
        "        count+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_Gpiik6P6SY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LujwxVSdP6SY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMG9bLBaP6SY"
      },
      "outputs": [],
      "source": [
        "#Reporting the accuracy of test set \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "data2 = pd.read_csv(\"A1_dataset.csv\")\n",
        "test_data = pd.read_csv(\"A2_test_dataset.csv\")\n",
        "def train_and_evaluate(train_sentences, train_labels,\n",
        "test_sentences, test_labels):\n",
        "\n",
        "    model = make_pipeline(TfidfVectorizer(), LogisticRegression())\n",
        "    model.fit(train_sentences, train_labels)\n",
        "    predicted_test_labels = model.predict(test_sentences)\n",
        "    return accuracy_score(test_labels, predicted_test_labels)\n",
        "\n",
        "train_sentences = data2['TEXT'].values\n",
        "train_labels = data2['LABEL'].values\n",
        "test_sentences = test_data['TEXT'].values\n",
        "test_labels = test_data['LABEL'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Os9ubtyvP6SY"
      },
      "source": [
        "## A.c Report the accuracy of test set using dataset A for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLRZ8wYAP6SY",
        "outputId": "1bc76c00-8af8-4ecd-c8c9-0f1ef18bd69f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8773291925465838"
            ]
          },
          "execution_count": 220,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_and_evaluate(train_sentences,train_labels,test_sentences,test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0G603RDiP6SZ"
      },
      "outputs": [],
      "source": [
        "##CREATING DATASET B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lc4u_v98P6SZ"
      },
      "outputs": [],
      "source": [
        "trial_data = data2[['LABEL','TEXT']] \n",
        "A = dict(zip(sentences,pred))\n",
        "index = trial_data.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoVmfwH0P6SZ"
      },
      "outputs": [],
      "source": [
        "for key,val in A.items():\n",
        "    lst = []\n",
        "    lst.append(val)\n",
        "    lst.append(key)\n",
        "    trial_data.loc[index] = lst\n",
        "    index+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMrzFRXbP6SZ",
        "outputId": "cffb713d-d884-449e-b51f-398f171d831f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4783, 2)"
            ]
          },
          "execution_count": 224,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trial_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGMvBdezP6SZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuTE2dWmP6SZ"
      },
      "outputs": [],
      "source": [
        "## B.e REPORT THE ACCURACY OF THE TEST SET USING DATASET B FOR TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ln1_szKEP6SZ",
        "outputId": "936cbe9c-8c7a-4ed5-c705-cb97c5476b8e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8819875776397516"
            ]
          },
          "execution_count": 226,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_and_evaluate(trial_data['TEXT'].values,trial_data['LABEL'].values,test_sentences,test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xdDJ7rtP6SZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DGTRe4DP6SZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RS8krJ2P6Sa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqAlo7sBP6Sa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2FlKe8QP6Sa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5ynLN0AP6Sa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}